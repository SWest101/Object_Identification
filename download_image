#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun  6 19:10:03 2018

@author: shaun
"""
from multiprocessing import Pool, Process
import requests
from bs4 import BeautifulSoup
import os
import shutil
import uuid


def download_images(url_list, output_path='./images/forklifts'):
    # loop through urls in list
    for url in url_list:
        # Split the list with the / delimiter
        url_split = url.split('/')

        # Creaate a unique image name using UUID
        img_name = '%s_%s' % (url_split[-1].split('.')[0], str(uuid.uuid1()))

        # Get the image extension
        img_ext = url_split[-1].split('.')[1]
        path = '%s/%s.%s' % (output_path, img_name[0:12], img_ext)

        # download the image as a stream
        try:
            img = requests.get(url, stream=True)
            if img.status_code == 200:
                with open(path, 'wb') as f:
                    img.raw.decode_content = True
                    shutil.copyfileobj(img.raw, f)
        except:
            print('error with URL: %s' % (url))


# split a list into evenly sized chunks
def chunks(l, n):
    return [l[i:i+n] for i in range(0, len(l), n)]

def dispatch_download(urls, job_number=os.cpu_count()):
    total = len(urls)
    chunk_size = total / job_number
    slices = chunks(urls, int(chunk_size))

    pool = Pool(job_number)
    pool.map(download_images, slices)
    pool.close()
    pool.join()


if __name__ == '__main__':
    # Collect and parse first page
    page = requests.get('http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03384352')
    soup = BeautifulSoup(page.text, 'html.parser')
    image_urls = soup.contents[0].split('\r\n')
    dispatch_download(image_urls)
